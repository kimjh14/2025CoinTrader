"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ 2025CoinTrader - ì•”í˜¸í™”í ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ëª¨ë“ˆ (data_collector.py)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ ì£¼ìš” ì—­í• :
    â€¢ ì—…ë¹„íŠ¸ APIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì•”í˜¸í™”í OHLCV ë°ì´í„° ìˆ˜ì§‘
    â€¢ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ë°ì´í„° ë™ì‹œ ìˆ˜ì§‘ (1ë¶„ë´‰~ì¼ë´‰)
    â€¢ 40+ ê¸°ìˆ ì  ì§€í‘œ ìë™ ê³„ì‚° ë° ì¶”ê°€
    â€¢ LSTM ëª¨ë¸ í›ˆë ¨ìš© ë ˆì´ë¸” ìƒì„± (ìƒìŠ¹/í•˜ë½/íš¡ë³´)
    â€¢ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì „ì²˜ë¦¬

ğŸ”§ í•µì‹¬ ê¸°ëŠ¥:
    1. ë°ì´í„° ìˆ˜ì§‘: ì—…ë¹„íŠ¸ REST APIë¥¼ í†µí•œ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘
    2. ê¸°ìˆ ì  ì§€í‘œ: MACD, RSI, ë³¼ë¦°ì €ë°´ë“œ, ì´ë™í‰ê· , ATR, ìŠ¤í† ìºìŠ¤í‹± ë“±
    3. ë©€í‹°í”„ë ˆì„: 1ë¶„, 3ë¶„, 5ë¶„, 15ë¶„, 1ì‹œê°„, 4ì‹œê°„, ì¼ë´‰ ë™ì‹œ ì²˜ë¦¬
    4. ë ˆì´ë¸”ë§: 5ë¶„ í›„ ê°€ê²© ë³€ë™ì„ ê¸°ì¤€ìœ¼ë¡œ 3-í´ë˜ìŠ¤ ë¶„ë¥˜ ë ˆì´ë¸” ìƒì„±
    5. ë°ì´í„° ìœµí•©: ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ë°ì´í„°ë¥¼ ì‹œê°„ ë™ê¸°í™”í•˜ì—¬ í†µí•©

ğŸ“Š ìƒì„±ë˜ëŠ” ë°ì´í„°:
    â€¢ {ì½”ì¸ëª…}_training_dataset.csv: LSTM í›ˆë ¨ìš© í†µí•© ë°ì´í„°ì…‹
    â€¢ ê¸°ìˆ ì  ì§€í‘œ: MACD, Signal, Histogram, RSI, BBìƒí•˜ë‹¨, MA5/10/20/60
    â€¢ ë³€ë™ì„± ì§€í‘œ: ATR, ê°€ê²© ë³€ë™ì„±, ê±°ë˜ëŸ‰ ë¹„ìœ¨
    â€¢ ì˜ˆì¸¡ ë ˆì´ë¸”: 0(ìƒìŠ¹), 1(í•˜ë½), 2(íš¡ë³´)

ğŸ“‹ ì‚¬ìš© ë°©ë²•:
    collector = CryptoDataCollector()
    data = collector.collect_data_for_coin('KRW-BTC', '2025-01-01', '2025-01-31')
    collector.save_training_data(data, 'BTC')

âš ï¸ ì£¼ì˜ì‚¬í•­:
    â€¢ API í˜¸ì¶œ ì œí•œì„ ì¤€ìˆ˜í•˜ì—¬ ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
    â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„ ë¡œì§ í¬í•¨
    â€¢ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì§„í–‰ìƒí™©ì„ ë¡œê·¸ë¡œ í™•ì¸ ê°€ëŠ¥

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import time
import pandas as pd
import numpy as np
import requests
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import json

# config.pyì—ì„œ API í‚¤ ê°€ì ¸ì˜¤
from config import Config

class CryptoDataCollector:
    """ê°€ìƒí™”í ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.config = Config()
        self.setup_logging()
        
        # ë¶„ì„ ëŒ€ìƒ ì½”ì¸ë“¤
        self.target_coins = ['KRW-BTC', 'KRW-DOGE', 'KRW-SOL', 'KRW-XRP', 'KRW-SUI', 'KRW-HBAR', 'KRW-APT']
        
        # íƒ€ì„í”„ë ˆì„ ì„¤ì •
        self.timeframes = {
            '1m': 1,
            '3m': 3, 
            '5m': 5,
            '15m': 15,
            '1h': 60,
            '4h': 240,
            '1d': 'D'
        }
        
        # ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ í™•ì¸)
        result_folder = os.environ.get('RESULT_FOLDER_PATH')
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        if result_folder and os.path.exists(result_folder):
            # ê²°ê³¼ í´ë”ê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ í´ë” ë‚´ì— DATA í´ë” ìƒì„±
            self.data_dir = os.path.join(result_folder, 'DATA')
        else:
            # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            self.data_dir = os.path.join(current_dir, 'DATA')
            
        self.raw_dir = os.path.join(self.data_dir, 'raw')
        self.processed_dir = os.path.join(self.data_dir, 'processed')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.create_directories()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê²°ê³¼ í´ë” ê²½ë¡œ í™•ì¸
        result_folder = os.environ.get('RESULT_FOLDER_PATH')
        
        if result_folder and os.path.exists(result_folder):
            # ê²°ê³¼ í´ë”ê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ í´ë”ì— ë¡œê·¸ ìƒì„±
            log_file_path = os.path.join(result_folder, 'data_collection.log')
        else:
            # ê¸°ë³¸ ê²½ë¡œì— ë¡œê·¸ ìƒì„±
            log_file_path = 'data_collection.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file_path, encoding='utf-8'),
                logging.StreamHandler()
            ],
            force=True  # ê¸°ì¡´ ë¡œê¹… ì„¤ì • ë®ì–´ì“°ê¸°
        )
        self.logger = logging.getLogger(__name__)
        
    def create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for directory in [self.data_dir, self.raw_dir, self.processed_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                self.logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")
                
    def collect_candle_data(self, market: str, timeframe: str, count: int = 200, to_date: str = None) -> pd.DataFrame:
        """ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ì—…ë¹„íŠ¸ API ì§ì ‘ í˜¸ì¶œ (to íŒŒë¼ë¯¸í„° ì§€ì›)
            if timeframe == "1" or timeframe == "3" or timeframe == "5" or timeframe == "10" or timeframe == "15" or timeframe == "30" or timeframe == "60" or timeframe == "240":
                target_url = f"https://api.upbit.com/v1/candles/minutes/{timeframe}"
            elif timeframe == "D":
                target_url = "https://api.upbit.com/v1/candles/days"
            elif timeframe == "W":
                target_url = "https://api.upbit.com/v1/candles/weeks"
            elif timeframe == "M":
                target_url = "https://api.upbit.com/v1/candles/months"
            else:
                raise Exception(f"ì˜ëª»ëœ í‹± ì¢…ë¥˜: {timeframe}")
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì„¤ì •
            params = {"market": market, "count": count}
            if to_date:
                # í•œêµ­ì‹œê°„ì„ UTCë¡œ ë³€í™˜ (ì—…ë¹„íŠ¸ APIëŠ” UTC ê¸°ì¤€)
                kst_datetime = datetime.strptime(to_date, '%Y-%m-%dT%H:%M:%S')
                utc_datetime = kst_datetime - timedelta(hours=9)
                params["to"] = utc_datetime.strftime('%Y-%m-%dT%H:%M:%S')
                # í•¨ìˆ˜ì— to_dateë¥¼ ë°›ì„ë• í•œêµ­ì‹œê°„ìœ¼ë¡œ ë°›ê³ , upbit apiëŠ” UTCì‹œê°„ìœ¼ë¡œ ë°›ê¸° ë•Œë¬¸ì— ë‚´ë¶€ì—ì„œ 9ì‹œê°„ ë¹¼ì¤Œ
            
            # API í˜¸ì¶œ
            response = requests.get(target_url, params=params)
            response.raise_for_status()
            candle_data = response.json()
            
            if not candle_data:
                self.logger.warning(f"{market} {timeframe} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(candle_data)
            
            # ì»¬ëŸ¼ëª… ì¤‘ë³µ í™•ì¸ ë° ì²˜ë¦¬
            if 'timestamp' in df.columns:
                df = df.drop(columns=['timestamp'])
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            df = df.rename(columns={
                'candle_date_time_kst': 'timestamp',
                'opening_price': 'open',
                'high_price': 'high', 
                'low_price': 'low',
                'trade_price': 'close',
                'candle_acc_trade_volume': 'volume',
                'candle_acc_trade_price': 'trade_price_volume'
            })
            
            # ì‹œê°„ìˆœ ì •ë ¬ (ìµœì‹ ì´ ìœ„ë¡œ)
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            return df
            
        except Exception as e:
            self.logger.error(f"{market} {timeframe} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()
    
    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        if df.empty:
            return df
            
        try:
            # MACD ê³„ì‚°
            exp1 = df['close'].ewm(span=12, adjust=False).mean()
            exp2 = df['close'].ewm(span=26, adjust=False).mean()
            df['macd'] = exp1 - exp2
            df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
            df['macd_histogram'] = df['macd'] - df['macd_signal']
            
            # RSI ê³„ì‚°
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))
            
            # ë³¼ë¦°ì € ë°´ë“œ ê³„ì‚°
            df['bb_middle'] = df['close'].rolling(window=20).mean()
            bb_std = df['close'].rolling(window=20).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
            df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
            
            # ì´ë™í‰ê· ì„ 
            df['ma5'] = df['close'].rolling(window=5).mean()
            df['ma10'] = df['close'].rolling(window=10).mean()
            df['ma20'] = df['close'].rolling(window=20).mean()
            df['ma60'] = df['close'].rolling(window=60).mean()
            
            # ATR (Average True Range) ê³„ì‚°
            high_low = df['high'] - df['low']
            high_close = np.abs(df['high'] - df['close'].shift())
            low_close = np.abs(df['low'] - df['close'].shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))
            df['atr'] = true_range.rolling(window=14).mean()
            
            # ê±°ë˜ëŸ‰ ê´€ë ¨ ì§€í‘œ
            df['volume_ma'] = df['volume'].rolling(window=20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_ma']
            
            # ê°€ê²© ë³€í™”ìœ¨
            df['price_change'] = df['close'].pct_change()
            df['price_change_5'] = df['close'].pct_change(periods=5)
            
            # ë³€ë™ì„± ì§€í‘œ
            df['volatility'] = df['close'].rolling(window=20).std() / df['close'].rolling(window=20).mean()
            
            self.logger.info("ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
            return df
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return df
    
    def extract_micro_features(self, df: pd.DataFrame, window_size: int = 60) -> pd.DataFrame:
        """ë¯¸ì„¸í•œ íŠ¹ì§• ì¶”ì¶œ (ì´ˆë´‰ ë°ì´í„°ë¥¼ í†µê³„ì  íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜)"""
        if df.empty or len(df) < window_size:
            return df
            
        try:
            # ìœˆë„ìš°ë³„ í†µê³„ì  íŠ¹ì§• ê³„ì‚°
            features = []
            
            for i in range(window_size, len(df)):
                window_data = df.iloc[i-window_size:i]
                
                feature_dict = {
                    'timestamp': df.iloc[i]['timestamp'],
                    'volatility': window_data['close'].std(),
                    'trend_strength': (window_data['close'].iloc[-1] - window_data['close'].iloc[0]) / window_data['close'].iloc[0],
                    'high_low_range': window_data['high'].max() - window_data['low'].min(),
                    'uptick_ratio': (window_data['close'].diff() > 0).sum() / window_size,
                    'volume_concentration': window_data['volume'].max() / window_data['volume'].mean(),
                    'price_momentum': window_data['close'].pct_change().mean(),
                    'volume_momentum': window_data['volume'].pct_change().mean()
                }
                features.append(feature_dict)
            
            micro_df = pd.DataFrame(features)
            self.logger.info(f"ë¯¸ì„¸ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(micro_df)}ê°œ")
            return micro_df
            
        except Exception as e:
            self.logger.error(f"ë¯¸ì„¸ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()
    
    def create_labels(self, df: pd.DataFrame, future_periods: int = 5, threshold: float = 0.002) -> pd.DataFrame:
        """ì˜ˆì¸¡ ëª©í‘œ ë ˆì´ë¸” ìƒì„± (3-class ë¶„ë¥˜)"""
        if df.empty:
            return df
            
        try:
            # ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
            df['future_return'] = df['close'].shift(-future_periods) / df['close'] - 1
            
            # 3-class ë ˆì´ë¸” ìƒì„±
            def create_label(return_val):
                if pd.isna(return_val):
                    return np.nan
                elif return_val > threshold:
                    return 0  # ìƒìŠ¹ (Buy)
                elif return_val < -threshold:
                    return 1  # í•˜ë½ (Sell)
                else:
                    return 2  # íš¡ë³´ (Hold)
            
            df['label'] = df['future_return'].apply(create_label)
            
            # ë ˆì´ë¸” ë¶„í¬ í™•ì¸
            label_counts = df['label'].value_counts()
            self.logger.info(f"ë ˆì´ë¸” ë¶„í¬: {label_counts.to_dict()}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"ë ˆì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return df
    
    def collect_all_data(self, days_back: int = 90, start_date=None, end_date=None):
        """ëª¨ë“  ì½”ì¸ì˜ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            days_back: ìˆ˜ì§‘í•  ì¼ìˆ˜ (start_dateê°€ ì—†ì„ ë•Œ ì‚¬ìš©)
            start_date: ì‹œì‘ ë‚ ì§œ (datetime ê°ì²´)
            end_date: ì¢…ë£Œ ë‚ ì§œ (datetime ê°ì²´)
        """
        if start_date and end_date:
            days_back = (end_date - start_date).days + 1
            self.logger.info(f"ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘... (ìˆ˜ì§‘ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}, {days_back}ì¼)")
        else:
            self.logger.info(f"ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘... (ìˆ˜ì§‘ ê¸°ê°„: {days_back}ì¼ = {days_back/30:.1f}ê°œì›”)")
        
        for coin in self.target_coins:
            self.logger.info(f"=== {coin} ë°ì´í„° ìˆ˜ì§‘ ===")
            
            for timeframe_name, timeframe_value in self.timeframes.items():
                try:
                    # ë°ì´í„° ìˆ˜ì§‘ (3ê°œì›”ì¹˜ ë°ì´í„° - API ì œí•œ ê³ ë ¤)
                    if timeframe_name == '1m':
                        # 1ë¶„ë´‰: 3ê°œì›” = 90ì¼ * 24ì‹œê°„ * 60ë¶„ = 129,600ê°œ
                        # API ì œí•œìœ¼ë¡œ 200ê°œì”© ì—¬ëŸ¬ ë²ˆ ìˆ˜ì§‘
                        count = 200
                    elif timeframe_name == '3m':
                        # 3ë¶„ë´‰: 3ê°œì›” = 90ì¼ * 24ì‹œê°„ * 20ë¶„ = 43,200ê°œ
                        count = 200
                    elif timeframe_name == '5m':
                        # 5ë¶„ë´‰: 3ê°œì›” = 90ì¼ * 24ì‹œê°„ * 12ë¶„ = 25,920ê°œ
                        count = 200
                    elif timeframe_name == '15m':
                        # 15ë¶„ë´‰: 3ê°œì›” = 90ì¼ * 24ì‹œê°„ * 4ë¶„ = 8,640ê°œ
                        count = 200
                    elif timeframe_name == '1h':
                        # 1ì‹œê°„ë´‰: 3ê°œì›” = 90ì¼ * 24ì‹œê°„ = 2,160ê°œ
                        count = 200
                    elif timeframe_name == '4h':
                        # 4ì‹œê°„ë´‰: 3ê°œì›” = 90ì¼ * 6ì‹œê°„ = 540ê°œ
                        count = 200
                    else:  # 1d
                        # ì¼ë´‰: 3ê°œì›” = 90ì¼
                        count = 90   # 90ì¼ (3ê°œì›”)
                    
                    # 3ê°œì›”ì¹˜ ë°ì´í„°ë¥¼ 200ê°œì”© ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜ì§‘ (ê³¼ê±°ë¶€í„° í˜„ì¬ê¹Œì§€)
                    all_data = []
                    current_date = None
                    total_collected = 0
                    
                    # ê° íƒ€ì„í”„ë ˆì„ë³„ë¡œ í•„ìš”í•œ ì´ ë°ì´í„° ìˆ˜ ê³„ì‚°
                    if timeframe_name == '1m':
                        total_needed = days_back * 24 * 60  # ì¼ìˆ˜ * 24ì‹œê°„ * 60ë¶„
                    elif timeframe_name == '3m':
                        total_needed = days_back * 24 * 20  # ì¼ìˆ˜ * 24ì‹œê°„ * 20ë¶„
                    elif timeframe_name == '5m':
                        total_needed = days_back * 24 * 12  # ì¼ìˆ˜ * 24ì‹œê°„ * 12ë¶„
                    elif timeframe_name == '15m':
                        total_needed = days_back * 24 * 4   # ì¼ìˆ˜ * 24ì‹œê°„ * 4ë¶„
                    elif timeframe_name == '1h':
                        total_needed = days_back * 24       # ì¼ìˆ˜ * 24ì‹œê°„
                    elif timeframe_name == '4h':
                        total_needed = days_back * 6        # ì¼ìˆ˜ * 6ì‹œê°„
                    else:  # 1d
                        total_needed = days_back            # ì¼ë´‰: ì¼ìˆ˜ë§Œí¼
                    
                    self.logger.info(f"{coin} {timeframe_name} {days_back}ì¼ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {total_needed}ê°œ)")
                    
                    # ë‚ ì§œ ì„¤ì •
                    from datetime import datetime, timedelta
                    if start_date and end_date:
                        # ì „ë‹¬ë°›ì€ ë‚ ì§œ ì‚¬ìš©
                        actual_start_date = start_date
                        actual_end_date = end_date
                    else:
                        # days_back ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
                        actual_end_date = datetime.now()
                        actual_start_date = actual_end_date - timedelta(days=days_back)
                    
                    # ê³¼ê±°ë¶€í„° í˜„ì¬ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜ì§‘
                    current_date = actual_start_date
                    last_progress_pct = 0
                    
                    while current_date <= actual_end_date and total_collected < total_needed:
                        # í˜„ì¬ ë‚ ì§œë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        to_date = current_date.strftime('%Y-%m-%dT%H:%M:%S')
                        
                        # 200ê°œì”© ìˆ˜ì§‘
                        df_chunk = self.collect_candle_data(coin, str(timeframe_value), 200, to_date)
                        
                        if not df_chunk.empty:
                            all_data.append(df_chunk)
                            total_collected += len(df_chunk)
                            
                            # ë‹¤ìŒ ë‚ ì§œë¡œ ì´ë™ (íƒ€ì„í”„ë ˆì„ì— ë”°ë¼)
                            if timeframe_name == '1m':
                                current_date += timedelta(minutes=200)
                            elif timeframe_name == '3m':
                                current_date += timedelta(minutes=600)
                            elif timeframe_name == '5m':
                                current_date += timedelta(minutes=1000)
                            elif timeframe_name == '15m':
                                current_date += timedelta(minutes=3000)
                            elif timeframe_name == '1h':
                                current_date += timedelta(hours=200)
                            elif timeframe_name == '4h':
                                current_date += timedelta(hours=800)
                            else:  # 1d
                                current_date += timedelta(days=200)
                            
                            # ì§„í–‰ë¥  ë¡œê·¸ë¥¼ 1% ë‹¨ìœ„ë¡œë§Œ ì¶œë ¥ (ë¡œê·¸ ì–‘ ì¤„ì´ê¸°)
                            progress_pct = int(total_collected / total_needed * 100)
                            if progress_pct > last_progress_pct and progress_pct % 1 == 0:
                                self.logger.info(f"{coin} {timeframe_name} ì§„í–‰ë¥ : {progress_pct}% ({total_collected}/{total_needed})")
                                last_progress_pct = progress_pct
                            
                            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                            time.sleep(0.1)
                        else:
                            self.logger.warning(f"{coin} {timeframe_name} {to_date} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            # ë‹¤ìŒ ë‚ ì§œë¡œ ì´ë™
                            if timeframe_name == '1d':
                                current_date += timedelta(days=1)
                            else:
                                current_date += timedelta(hours=1)
                    
                    # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                    if all_data:
                        df = pd.concat(all_data, ignore_index=True)
                        df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
                        self.logger.info(f"{coin} {timeframe_name} ì´ {len(df)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ëª©í‘œ ëŒ€ë¹„ {len(df)/total_needed*100:.1f}%)")
                    else:
                        df = pd.DataFrame()
                        self.logger.error(f"{coin} {timeframe_name} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                    
                    if not df.empty:
                        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                        df = self.calculate_technical_indicators(df)
                        
                        # ë¯¸ì„¸ íŠ¹ì§• ì¶”ì¶œ (1ë¶„ë´‰ì˜ ê²½ìš°)
                        if timeframe_name == '1m':
                            micro_features = self.extract_micro_features(df)
                            if not micro_features.empty:
                                # ë¯¸ì„¸ íŠ¹ì§•ì„ ë©”ì¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
                                df = df.merge(micro_features, on='timestamp', how='left', suffixes=('', '_micro'))
                        
                        # ë ˆì´ë¸” ìƒì„±
                        df = self.create_labels(df)
                        
                        # ë°ì´í„° ì €ì¥
                        filename = f"{coin.replace('KRW-', '')}_{timeframe_name}.csv"
                        filepath = f"{self.raw_dir}/{filename}"
                        df.to_csv(filepath, index=False)
                        
                        self.logger.info(f"{filename} ì €ì¥ ì™„ë£Œ")
                        
                        # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                        time.sleep(0.1)
                        
                except Exception as e:
                    self.logger.error(f"{coin} {timeframe_name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    continue
    
    def analyze_data_quality(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        self.logger.info("ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì‹œì‘...")
        
        analysis_results = {}
        
        for coin in self.target_coins:
            coin_name = coin.replace('KRW-', '')
            analysis_results[coin_name] = {}
            
            for timeframe_name in self.timeframes.keys():
                filename = f"{coin_name}_{timeframe_name}.csv"
                filepath = f"{self.raw_dir}/{filename}"
                
                if os.path.exists(filepath):
                    df = pd.read_csv(filepath)
                    
                    analysis = {
                        'total_rows': len(df),
                        'missing_values': df.isnull().sum().to_dict(),
                        'date_range': {
                            'start': df['timestamp'].min() if 'timestamp' in df.columns else 'N/A',
                            'end': df['timestamp'].max() if 'timestamp' in df.columns else 'N/A'
                        },
                        'price_stats': {
                            'min': df['close'].min() if 'close' in df.columns else 'N/A',
                            'max': df['close'].max() if 'close' in df.columns else 'N/A',
                            'mean': df['close'].mean() if 'close' in df.columns else 'N/A'
                        }
                    }
                    
                    analysis_results[coin_name][timeframe_name] = analysis
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        with open(f"{self.processed_dir}/data_quality_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info("ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
        return analysis_results
    
    def create_training_dataset(self):
        """LSTM í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±"""
        self.logger.info("í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        
        # 1ë¶„ë´‰ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë©€í‹° íƒ€ì„í”„ë ˆì„ ë°ì´í„°ì…‹ ìƒì„±
        for coin in self.target_coins:
            coin_name = coin.replace('KRW-', '')
            
            # 1ë¶„ë´‰ ë°ì´í„° ë¡œë“œ
            df_1m = pd.read_csv(f"{self.raw_dir}/{coin_name}_1m.csv")
            df_1m['timestamp'] = pd.to_datetime(df_1m['timestamp'])
            
            # ë‹¤ë¥¸ íƒ€ì„í”„ë ˆì„ ë°ì´í„° ë¡œë“œ ë° ë³‘í•©
            for timeframe_name in ['5m', '15m', '1h']:
                try:
                    df_other = pd.read_csv(f"{self.raw_dir}/{coin_name}_{timeframe_name}.csv")
                    df_other['timestamp'] = pd.to_datetime(df_other['timestamp'])
                    
                    # ì»¬ëŸ¼ëª…ì— íƒ€ì„í”„ë ˆì„ ì ‘ë¯¸ì‚¬ ì¶”ê°€
                    suffix_cols = ['close', 'volume', 'rsi', 'macd', 'bb_middle', 'atr']
                    rename_dict = {col: f"{col}_{timeframe_name}" for col in suffix_cols if col in df_other.columns}
                    df_other = df_other.rename(columns=rename_dict)
                    
                    # 1ë¶„ë´‰ ë°ì´í„°ì— ë³‘í•©
                    df_1m = df_1m.merge(
                        df_other[['timestamp'] + list(rename_dict.values())], 
                        on='timestamp', 
                        how='left'
                    )
                    
                except Exception as e:
                    self.logger.warning(f"{coin_name} {timeframe_name} ë³‘í•© ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # ìµœì¢… í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥
            training_filepath = f"{self.processed_dir}/{coin_name}_training_dataset.csv"
            df_1m.to_csv(training_filepath, index=False)
            
            self.logger.info(f"{coin_name} í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(df_1m)}í–‰")
        
        self.logger.info("í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = CryptoDataCollector()
    
    # 1. ë°ì´í„° ìˆ˜ì§‘
    collector.collect_all_data(days_back=30)
    
    # 2. ë°ì´í„° í’ˆì§ˆ ë¶„ì„
    analysis_results = collector.analyze_data_quality()
    
    # 3. í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±
    collector.create_training_dataset()
    
    print("ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()