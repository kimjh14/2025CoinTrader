"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤– 2025CoinTrader - ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ Bi-LSTM ëª¨ë¸ í›ˆë ¨ ëª¨ë“ˆ (Training_v04.py)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ ì£¼ìš” ì—­í• :
    â€¢ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ Bi-LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬í˜„ ë° í›ˆë ¨
    â€¢ 1ë¶„ë´‰ + 5ë¶„ë´‰ MACD ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ 1ë¶„ í›„ ê°€ê²© ì˜ˆì¸¡
    â€¢ GPU ê°€ì† ë° ë©€í‹° í”„ë¡œì„¸ì‹± ìµœì í™”
    â€¢ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”
    â€¢ í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬

ğŸ”§ í•µì‹¬ ê¸°ëŠ¥:
    1. ëª¨ë¸ ì•„í‚¤í…ì²˜: Bidirectional LSTM + Multi-head Attention
    2. ë°ì´í„° ì²˜ë¦¬: ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ ë°ì´í„° ì‹œê°„ ë™ê¸°í™” ë° ì •ê·œí™”
    3. í´ë˜ìŠ¤ ê· í˜•: ë¶ˆê· í˜• ë°ì´í„°ë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ì ìš©
    4. ì„±ëŠ¥ ìµœì í™”: GPU ê°€ì†, ë°°ì¹˜ ì²˜ë¦¬, ì¡°ê¸° ì¢…ë£Œ
    5. ê²°ê³¼ ë¶„ì„: í˜¼ë™ í–‰ë ¬, F1 ì ìˆ˜, ì •í™•ë„ í‰ê°€

ğŸ§  ëª¨ë¸ êµ¬ì¡°:
    ì…ë ¥: 10ê°œ íŠ¹ì§• Ã— 10 ì‹œí€€ìŠ¤ ê¸¸ì´
    - 1ë¶„ë´‰ MACD (MACD, Signal, Histogram)
    - 5ë¶„ë´‰ MACD (MACD, Signal, Histogram) 
    - ë‹¤ì´ë²„ì „ìŠ¤ íŠ¹ì§• (2ê°œ)
    - ê°€ê²© ë³€í™”ìœ¨ (2ê°œ)
    
    ì•„í‚¤í…ì²˜:
    - Bi-LSTM Layer 1: 128 hidden units
    - Bi-LSTM Layer 2: 128 hidden units  
    - Multi-head Attention: 4 heads
    - Dense Layer: 64 units (ReLU)
    - Output Layer: 3 classes (Softmax)

ğŸ“Š í›ˆë ¨ ì„¤ì •:
    â€¢ í›ˆë ¨ ì½”ì¸: APT, BTC, DOGE, HBAR, SOL, XRP
    â€¢ ê²€ì¦ ì½”ì¸: SUI (Out-of-sample í…ŒìŠ¤íŠ¸)
    â€¢ ë°°ì¹˜ í¬ê¸°: 512 (GPU) / 64 (CPU)
    â€¢ í•™ìŠµë¥ : 0.001 (Adam ì˜µí‹°ë§ˆì´ì €)
    â€¢ ì¡°ê¸° ì¢…ë£Œ: 15 ì—í¬í¬ patience

ğŸ“‹ ì‚¬ìš© ë°©ë²•:
    python Training_v04.py
    
    ìƒì„± íŒŒì¼:
    - multitimeframe_bilstm_crypto_model_v04.pth: ìµœì¢… ëª¨ë¸
    - best_multitimeframe_model_v04.pth: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    - multitimeframe_preprocessor_v04.pkl: ì „ì²˜ë¦¬ê¸°
    - model_info_v04.pkl: ëª¨ë¸ ë©”íƒ€ë°ì´í„°

âš ï¸ ì£¼ì˜ì‚¬í•­:
    â€¢ GPU ë©”ëª¨ë¦¬ 4GB ì´ìƒ ê¶Œì¥ (CUDA 11.8+)
    â€¢ ë°ì´í„° íŒŒì¼ì´ data/raw/ í´ë”ì— ìˆì–´ì•¼ í•¨
    â€¢ í›ˆë ¨ ì‹œê°„: GPU ê¸°ì¤€ 30ë¶„~2ì‹œê°„ ì†Œìš”

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import pickle
import os
import multiprocessing
import logging
from tqdm import tqdm

# GPU/CPU ìµœì í™” ì„¤ì •
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    device = 'cuda'
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
    batch_size = 256
else:
    torch.set_num_threads(multiprocessing.cpu_count())
    device = 'cpu'
    print(f"Using {torch.get_num_threads()} CPU threads for PyTorch")
    batch_size = 128

print(f"CPU cores: {multiprocessing.cpu_count()}")
print(f"PyTorch threads: {torch.get_num_threads()}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MultiTimeframePreprocessor:
    """
    ë‹¤ì¤‘ ì‹œê°„ëŒ€ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
    - 1ë¶„ë´‰ + 5ë¶„ë´‰ MACD íŠ¹ì§•
    - 0.05% ì„ê³„ê°’ ì‚¬ìš©
    - ì¸í„°í´ë ˆì´ì…˜ì„ í†µí•œ ì‹œê°„ ì •ë ¬
    """
    
    def __init__(self, sequence_length: int = 10):
        self.sequence_length = sequence_length
        self.scaler = StandardScaler()
        self.threshold = 0.05  # 0.05%
        
    def calculate_macd(self, data: pd.Series, 
                      fast_period: int = 12,
                      slow_period: int = 26,
                      signal_period: int = 9) -> Dict[str, pd.Series]:
        """MACD ì§€í‘œ ê³„ì‚°"""
        ema_fast = data.ewm(span=fast_period, adjust=False).mean()
        ema_slow = data.ewm(span=slow_period, adjust=False).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()
        macd_histogram = macd_line - signal_line
        
        # ì •ê·œí™”
        macd_line = (macd_line - macd_line.mean()) / (macd_line.std() + 1e-8)
        signal_line = (signal_line - signal_line.mean()) / (signal_line.std() + 1e-8)
        macd_histogram = (macd_histogram - macd_histogram.mean()) / (macd_histogram.std() + 1e-8)
        
        return {
            'macd': macd_line,
            'signal': signal_line,
            'histogram': macd_histogram
        }
    
    def create_labels(self, data: pd.DataFrame) -> pd.Series:
        """
        3-í´ë˜ìŠ¤ ë ˆì´ë¸” ìƒì„±
        - í´ë˜ìŠ¤ 0 (í•˜ë½): 1ë¶„ ë’¤ ìˆ˜ìµë¥  < -0.05%
        - í´ë˜ìŠ¤ 1 (ë³´ë¥˜): -0.05% â‰¤ 1ë¶„ ë’¤ ìˆ˜ìµë¥  â‰¤ 0.05%
        - í´ë˜ìŠ¤ 2 (ìƒìŠ¹): 1ë¶„ ë’¤ ìˆ˜ìµë¥  > 0.05%
        """
        returns = (data['close'].shift(-1) - data['close']) / data['close'] * 100
        labels = pd.Series(1, index=data.index)  # ê¸°ë³¸ê°’: ë³´ë¥˜
        labels[returns < -self.threshold] = 0  # í•˜ë½
        labels[returns > self.threshold] = 2   # ìƒìŠ¹
        
        # í´ë˜ìŠ¤ ë¶„í¬ ë¡œê¹…
        class_dist = labels.value_counts(normalize=True).sort_index()
        logger.info(f"ë ˆì´ë¸” ë¶„í¬ (Â±{self.threshold}%): í•˜ë½={class_dist.get(0, 0):.2%}, ë³´ë¥˜={class_dist.get(1, 0):.2%}, ìƒìŠ¹={class_dist.get(2, 0):.2%}")
        
        # ê±°ë˜ ì‹ í˜¸ ê°œìˆ˜ ê³„ì‚°
        trading_signals = len(labels[labels != 1])
        logger.info(f"ìƒì„±ëœ ê±°ë˜ ì‹ í˜¸: {trading_signals}ê°œ ({trading_signals/len(labels)*100:.2%})")
        
        return labels
    
    def prepare_multi_timeframe_features(self, data_1m: pd.DataFrame, data_5m: pd.DataFrame) -> pd.DataFrame:
        """1ë¶„ë´‰ + 5ë¶„ë´‰ MACD íŠ¹ì§• ìƒì„±"""
        features = pd.DataFrame(index=data_1m.index)
        
        # 1ë¶„ë´‰ MACD (ë¹ ë¥¸ ë°˜ì‘)
        macd_1m = self.calculate_macd(data_1m['close'], fast_period=8, slow_period=17, signal_period=5)
        features['macd_1m'] = macd_1m['macd']
        features['macd_signal_1m'] = macd_1m['signal']
        features['macd_histogram_1m'] = macd_1m['histogram']
        
        # 5ë¶„ë´‰ ë°ì´í„°ë¥¼ 1ë¶„ë´‰ ì¸ë±ìŠ¤ì— ë§ì¶° ë¦¬ìƒ˜í”Œë§ (ì¸í„°í´ë ˆì´ì…˜)
        # 5ë¶„ë´‰ MACD ê³„ì‚°
        macd_5m = self.calculate_macd(data_5m['close'])
        
        # 5ë¶„ë´‰ MACDë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        macd_5m_df = pd.DataFrame({
            'macd_5m': macd_5m['macd'],
            'macd_signal_5m': macd_5m['signal'],
            'macd_histogram_5m': macd_5m['histogram']
        }, index=data_5m.index)
        
        # 1ë¶„ ê°„ê²©ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ (ì„ í˜• ë³´ê°„)
        macd_5m_resampled = macd_5m_df.resample('1min').interpolate(method='linear')
        
        # 1ë¶„ë´‰ ì¸ë±ìŠ¤ì— ë§ì¶° ì •ë ¬
        macd_5m_aligned = macd_5m_resampled.reindex(data_1m.index)
        
        # Forward fillê³¼ backward fillë¡œ ë‚¨ì€ NaN ì²˜ë¦¬
        macd_5m_aligned = macd_5m_aligned.fillna(method='ffill').fillna(method='bfill')
        
        # 5ë¶„ë´‰ MACD íŠ¹ì§• ì¶”ê°€
        features['macd_5m'] = macd_5m_aligned['macd_5m']
        features['macd_signal_5m'] = macd_5m_aligned['macd_signal_5m']
        features['macd_histogram_5m'] = macd_5m_aligned['macd_histogram_5m']
        
        # ì¶”ê°€: 1ë¶„ë´‰ê³¼ 5ë¶„ë´‰ MACDì˜ ì°¨ì´ (ë‹¤ì´ë²„ì „ìŠ¤ í¬ì°©)
        features['macd_divergence'] = features['macd_1m'] - features['macd_5m']
        features['histogram_divergence'] = features['macd_histogram_1m'] - features['macd_histogram_5m']
        
        # ì¶”ê°€: ê°€ê²© ë³€í™”ìœ¨
        features['price_change'] = data_1m['close'].pct_change() * 100
        features['price_change_ma'] = features['price_change'].rolling(5).mean()
        
        # NaN ì²˜ë¦¬
        features = features.bfill().ffill()
        
        logger.info(f"ë‹¤ì¤‘ ì‹œê°„ëŒ€ íŠ¹ì§• ìƒì„± ì™„ë£Œ: {features.shape[1]}ê°œ íŠ¹ì§•")
        
        return features
    
    def create_sequences(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        sequences = []
        sequence_labels = []
        
        for i in range(self.sequence_length, len(features)):
            sequences.append(features[i-self.sequence_length:i])
            sequence_labels.append(labels[i])
        
        return np.array(sequences), np.array(sequence_labels)
    
    def split_data(self, features: pd.DataFrame, labels: pd.Series, 
                   train_ratio: float = 0.7, val_ratio: float = 0.15) -> Dict:
        """ë°ì´í„° Train/Validation/Test ë¶„í• """
        # DataFrame -> numpy ë³€í™˜
        features_np = features.values
        labels_np = labels.values
        
        # NaN ì²˜ë¦¬
        features_np = np.nan_to_num(features_np, nan=0.0)
        
        n_samples = len(labels_np)
        logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {n_samples}")
        
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))
        
        # ì •ê·œí™”
        X_train = features_np[:train_end]
        X_val = features_np[train_end:val_end]
        X_test = features_np[val_end:]
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)

        y_train = labels_np[:train_end]
        y_val = labels_np[train_end:val_end]
        y_test = labels_np[val_end:]

        # LSTMìš© ì‹œí€€ìŠ¤ ë°ì´í„°
        train_sequences, train_labels = self.create_sequences(X_train_scaled, y_train)
        val_sequences, val_labels = self.create_sequences(X_val_scaled, y_val)
        test_sequences, test_labels = self.create_sequences(X_test_scaled, y_test)

        return {
            'X_train': train_sequences,
            'y_train': train_labels,
            'X_val': val_sequences,
            'y_val': val_labels,
            'X_test': test_sequences,
            'y_test': test_labels,
            'feature_names': list(features.columns)
        }


class MultiTimeframeBiLSTM(nn.Module):
    """ë‹¤ì¤‘ ì‹œê°„ëŒ€ Bi-LSTM ëª¨ë¸"""
    
    def __init__(self, input_size: int = 10, hidden_size: int = 128, num_layers: int = 2, 
                 num_classes: int = 3, dropout: float = 0.2):
        super(MultiTimeframeBiLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Bi-LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        
        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Softmax(dim=1)
        )
        
        # Multi-head attention for timeframe features
        self.timeframe_attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,
            num_heads=4,
            dropout=dropout
        )
        
        # ì¶œë ¥ì¸µ
        self.dropout = nn.Dropout(dropout)
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        # LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(x)
        
        # Self-attention
        attention_weights = self.attention(lstm_out)
        context = torch.sum(attention_weights * lstm_out, dim=1)
        
        # Dropout and fully connected
        out = self.dropout(context)
        out = F.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.fc2(out)
        
        return out


class MultiTimeframeLSTMPredictor:
    """
    ë‹¤ì¤‘ ì‹œê°„ëŒ€ Bi-LSTM ëª¨ë¸ í´ë˜ìŠ¤
    - 1ë¶„ë´‰ + 5ë¶„ë´‰ MACD íŠ¹ì§• í™œìš©
    - ê°œì„ ëœ ì˜ˆì¸¡ ì •í™•ë„ ëª©í‘œ
    """
    
    def __init__(self, input_size: int = 10, 
                 hidden_size: int = 128, num_layers: int = 2, 
                 num_classes: int = 3, learning_rate: float = 0.001):
        self.device = torch.device(device)
        self.num_classes = num_classes
        self.probability_threshold = 0.45  # ë” ë³´ìˆ˜ì ì¸ ì„ê³„ê°’
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.model = MultiTimeframeBiLSTM(input_size, hidden_size, num_layers, num_classes).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        # ëª¨ë¸ì„ DataParallelë¡œ ê°ì‹¸ì„œ ë©€í‹° GPU ì§€ì›
        if torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            logger.info(f"Multi-GPU enabled: {torch.cuda.device_count()} GPUs")
        
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: np.ndarray, y_val: np.ndarray,
              epochs: int = 50, batch_size: int = 256) -> Dict:
        """ëª¨ë¸ í›ˆë ¨"""
        # ë³‘ë ¬ì²˜ë¦¬ ìµœì í™” ì„¤ì •
        if torch.cuda.is_available():
            num_workers = min(8, multiprocessing.cpu_count())
            pin_memory = True
        else:
            num_workers = 0
            pin_memory = False
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=True if num_workers > 0 else False
        )
        
        # ê²€ì¦ ë°ì´í„°
        X_val_tensor = torch.FloatTensor(X_val).to(self.device, non_blocking=True)
        y_val_tensor = torch.LongTensor(y_val).to(self.device, non_blocking=True)
        
        # ê· í˜•ì¡íŒ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(y_train),
            y=y_train
        )
        class_weights_tensor = torch.FloatTensor(class_weights).to(self.device)
        self.criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
        
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        unique, counts = np.unique(y_train, return_counts=True)
        logger.info(f"í›ˆë ¨ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬: {dict(zip(unique, counts))}")
        logger.info(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights}")
        
        # Scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=5
        )
        
        # í›ˆë ¨ ê¸°ë¡
        train_losses = []
        val_losses = []
        val_accuracies = []
        val_signal_ratios = []
        val_f1_scores = []
        
        logger.info(f"ë‹¤ì¤‘ ì‹œê°„ëŒ€ Bi-LSTM í›ˆë ¨ ì‹œì‘: {epochs} epochs")
        
        # Training loop
        best_f1_score = 0
        patience_counter = 0
        patience = 10
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0
            
            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')
            for batch_X, batch_y in train_pbar:
                batch_X = batch_X.to(self.device, non_blocking=True)
                batch_y = batch_y.to(self.device, non_blocking=True)
                
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                train_loss += loss.item()
                
                train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
            
            # Validation
            self.model.eval()
            val_loss = 0
            all_val_preds = []
            all_val_probs = []
            
            with torch.no_grad():
                val_batch_size = batch_size * 2
                for i in range(0, len(X_val_tensor), val_batch_size):
                    end_idx = min(i + val_batch_size, len(X_val_tensor))
                    
                    batch_X = X_val_tensor[i:end_idx]
                    batch_y = y_val_tensor[i:end_idx]
                    
                    val_outputs = self.model(batch_X)
                    val_loss += self.criterion(val_outputs, batch_y).item()
                    
                    # ì˜ˆì¸¡
                    _, preds = torch.max(val_outputs, 1)
                    all_val_preds.extend(preds.cpu().numpy())
                    
                    # í™•ë¥ 
                    probs = torch.softmax(val_outputs, dim=1)
                    all_val_probs.extend(probs.cpu().numpy())
            
            # í‰ê°€ ì§€í‘œ ê³„ì‚°
            val_accuracy = accuracy_score(y_val, all_val_preds)
            val_f1 = f1_score(y_val, all_val_preds, average='macro')
            signal_ratio = np.sum(np.array(all_val_preds) != 1) / len(all_val_preds)
            
            train_losses.append(train_loss / len(train_loader))
            val_losses.append(val_loss / ((len(X_val_tensor) + val_batch_size - 1) // val_batch_size))
            val_accuracies.append(val_accuracy)
            val_f1_scores.append(val_f1)
            val_signal_ratios.append(signal_ratio)
            
            # Log results
            logger.info(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, '
                       f'Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, '
                       f'Signal Ratio: {signal_ratio:.2%}')
            
            # F1 score ê¸°ë°˜ ì €ì¥
            if val_f1 > best_f1_score:
                best_f1_score = val_f1
                patience_counter = 0
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'probability_threshold': self.probability_threshold
                }, 'best_multitimeframe_model_v04.pth')
                logger.info(f"Best model saved with F1 score: {best_f1_score:.4f}")
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                logger.info("Early stopping triggered")
                break
            
            # Scheduler step
            scheduler.step(val_f1)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies,
            'val_signal_ratios': val_signal_ratios,
            'val_f1_scores': val_f1_scores
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡"""
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            batch_size = 256
            for i in range(0, len(X), batch_size):
                end_idx = min(i + batch_size, len(X))
                
                batch_X = torch.FloatTensor(X[i:end_idx]).to(self.device, non_blocking=True)
                outputs = self.model(batch_X)
                _, preds = torch.max(outputs, 1)
                predictions.extend(preds.cpu().numpy())
        
        return np.array(predictions)
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡ í™•ë¥ """
        self.model.eval()
        probabilities = []
        
        with torch.no_grad():
            batch_size = 256
            for i in range(0, len(X), batch_size):
                end_idx = min(i + batch_size, len(X))
                
                batch_X = torch.FloatTensor(X[i:end_idx]).to(self.device, non_blocking=True)
                outputs = self.model(batch_X)
                proba = torch.softmax(outputs, dim=1)
                probabilities.extend(proba.cpu().numpy())
        
        return np.array(probabilities)
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """ëª¨ë¸ í‰ê°€"""
        y_pred = self.predict(X_test)
        
        # ê±°ë˜ ì‹ í˜¸ í†µê³„
        signal_count = np.sum(y_pred != 1)
        signal_ratio = signal_count / len(y_pred)
        
        accuracy = accuracy_score(y_test, y_pred)
        f1_scores = f1_score(y_test, y_pred, average=None)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        conf_matrix = confusion_matrix(y_test, y_pred)
        
        return {
            'accuracy': accuracy,
            'f1_scores': f1_scores,
            'f1_macro': f1_macro,
            'confusion_matrix': conf_matrix,
            'signal_count': signal_count,
            'signal_ratio': signal_ratio,
            'classification_report': classification_report(y_test, y_pred)
        }
    
    def save_model(self, filepath: str) -> None:
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'probability_threshold': self.probability_threshold
        }, filepath)


def load_and_filter_data(coin: str, start_date: str, end_date: str, 
                        timeframe: str = '1m', data_dir: str = None) -> pd.DataFrame:
    """ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ í•„í„°ë§"""
    if data_dir is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(os.path.dirname(current_dir), 'Data_maker', 'DATA', 'raw')
    filepath = os.path.join(data_dir, f'{coin}_{timeframe}.csv')
    df = pd.read_csv(filepath)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.set_index('timestamp')
    
    # ë‚ ì§œ í•„í„°ë§
    df = df[(df.index >= start_date) & (df.index <= end_date)]
    
    return df


def main():
    """ë©”ì¸ ì‹¤í–‰ ì½”ë“œ"""
    # ì„¤ì •
    START_DATE = '2025-05-03'
    END_DATE = '2025-07-30'
    COINS = ['APT', 'BTC', 'DOGE', 'HBAR', 'SOL', 'XRP']  # í›ˆë ¨ìš©
    VAL_COIN = 'SUI'  # ê²€ì¦ìš©
    
    logger.info("=== ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì•”í˜¸í™”í ì˜ˆì¸¡ ëª¨ë¸ V04 í›ˆë ¨ ì‹œì‘ ===")
    logger.info(f"í›ˆë ¨ ì½”ì¸: {COINS}")
    logger.info(f"ê²€ì¦ ì½”ì¸: {VAL_COIN}")
    logger.info(f"ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    logger.info("íŠ¹ì§•: 1ë¶„ë´‰ + 5ë¶„ë´‰ MACD, ì¸í„°í´ë ˆì´ì…˜ ì ìš©")
    
    print("\n1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°ì´í„° ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = MultiTimeframePreprocessor(sequence_length=10)
    
    # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
    logger.info("í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    # ê° ì½”ì¸ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬ ë° ê²°í•©
    all_coin_data = []
    total_signals = 0
    
    for coin in tqdm(COINS, desc="ì½”ì¸ ë°ì´í„° ì²˜ë¦¬"):
        logger.info(f"\n   - {coin} ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        # 1ë¶„ë´‰ê³¼ 5ë¶„ë´‰ ë°ì´í„° ë¡œë“œ
        data_1m = load_and_filter_data(coin, START_DATE, END_DATE, '1m')
        data_5m = load_and_filter_data(coin, START_DATE, END_DATE, '5m')
        
        logger.info(f"     1ë¶„ë´‰: {len(data_1m)}ê°œ, 5ë¶„ë´‰: {len(data_5m)}ê°œ")
        
        # ë‹¤ì¤‘ ì‹œê°„ëŒ€ íŠ¹ì§• ìƒì„±
        features = preprocessor.prepare_multi_timeframe_features(data_1m, data_5m)
        labels = preprocessor.create_labels(data_1m)
        
        # ê±°ë˜ ì‹ í˜¸ ìˆ˜ í™•ì¸
        coin_signals = len(labels[labels != 1])
        total_signals += coin_signals
        logger.info(f"   {coin} ê±°ë˜ ì‹ í˜¸: {coin_signals}ê°œ")
        
        # ê° ì½”ì¸ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
        coin_data = {
            'features': features,
            'labels': labels,
            'coin': coin
        }
        all_coin_data.append(coin_data)
    
    logger.info(f"\nì´ í›ˆë ¨ ê±°ë˜ ì‹ í˜¸: {total_signals}ê°œ")
    
    # ëª¨ë“  ì½”ì¸ì˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•©
    train_features_list = []
    train_labels_list = []
    
    for coin_data in all_coin_data:
        train_features_list.append(coin_data['features'])
        train_labels_list.append(coin_data['labels'])
    
    # ê²°í•©
    train_features = pd.concat(train_features_list, axis=0, ignore_index=True)
    train_labels = pd.concat(train_labels_list, axis=0, ignore_index=True)
    
    logger.info(f"í›ˆë ¨ ë°ì´í„° ê²°í•© ì™„ë£Œ: {train_features.shape}")
    
    # ê²€ì¦ ë°ì´í„° ì¤€ë¹„
    logger.info(f"\n   - {VAL_COIN} ê²€ì¦ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    val_data_1m = load_and_filter_data(VAL_COIN, START_DATE, END_DATE, '1m')
    val_data_5m = load_and_filter_data(VAL_COIN, START_DATE, END_DATE, '5m')
    
    val_features = preprocessor.prepare_multi_timeframe_features(val_data_1m, val_data_5m)
    val_labels = preprocessor.create_labels(val_data_1m)
    
    val_signals = len(val_labels[val_labels != 1])
    logger.info(f"   {VAL_COIN} ê±°ë˜ ì‹ í˜¸: {val_signals}ê°œ")
    logger.info(f"ê²€ì¦ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {val_features.shape}")
    
    # ë°ì´í„° ë¶„í• 
    logger.info("\në°ì´í„° ë¶„í•  ì‹œì‘...")
    data_splits = preprocessor.split_data(train_features, train_labels)
    
    # ê²€ì¦ ë°ì´í„° ì²˜ë¦¬
    val_features_scaled = preprocessor.scaler.transform(val_features)
    val_sequences, val_seq_labels = preprocessor.create_sequences(val_features_scaled, val_labels.values)
    
    logger.info("\n2. ë‹¤ì¤‘ ì‹œê°„ëŒ€ Bi-LSTM ëª¨ë¸ í›ˆë ¨...")
    input_size = len(data_splits['feature_names'])  # 10ê°œ íŠ¹ì§• (1ë¶„ë´‰ MACD 3 + 5ë¶„ë´‰ MACD 3 + ë‹¤ì´ë²„ì „ìŠ¤ 2 + ê°€ê²©ë³€í™” 2)
    logger.info(f"ì…ë ¥ íŠ¹ì§• ìˆ˜: {input_size}")
    logger.info(f"íŠ¹ì§• ì´ë¦„: {data_splits['feature_names']}")
    
    lstm_model = MultiTimeframeLSTMPredictor(input_size=input_size)
    
    training_history = lstm_model.train(
        data_splits['X_train'],
        data_splits['y_train'],
        data_splits['X_val'],
        data_splits['y_val'],
        epochs=50,
        batch_size=batch_size
    )
    
    # ëª¨ë¸ í‰ê°€
    lstm_eval = lstm_model.evaluate(val_sequences, val_seq_labels)
    logger.info(f"\n   ë‹¤ì¤‘ ì‹œê°„ëŒ€ Bi-LSTM Accuracy: {lstm_eval['accuracy']:.4f}")
    logger.info(f"   ê±°ë˜ ì‹ í˜¸ ìˆ˜: {lstm_eval['signal_count']}ê°œ")
    logger.info(f"   ê±°ë˜ ì‹ í˜¸ ë¹„ìœ¨: {lstm_eval['signal_ratio']:.2%}")
    logger.info(f"   Macro F1 Score: {lstm_eval['f1_macro']:.4f}")
    logger.info(f"   F1-scores: {lstm_eval['f1_scores']}")
    print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
    print(lstm_eval['classification_report'])
    
    # ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
    predictions = lstm_model.predict(val_sequences)
    pred_dist = pd.Series(predictions).value_counts(normalize=True).sort_index()
    logger.info(f"\nì˜ˆì¸¡ ë¶„í¬: í•˜ë½={pred_dist.get(0, 0):.2%}, ë³´ë¥˜={pred_dist.get(1, 0):.2%}, ìƒìŠ¹={pred_dist.get(2, 0):.2%}")
    
    # ì‹œê°í™”
    logger.info("\n3. ê²°ê³¼ ì‹œê°í™”...")
    
    # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì‹œê°í™”
    plt.figure(figsize=(20, 5))
    
    plt.subplot(1, 4, 1)
    plt.plot(training_history['train_losses'], label='Train Loss')
    plt.plot(training_history['val_losses'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 4, 2)
    plt.plot(training_history['val_accuracies'], label='Validation Accuracy')
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 4, 3)
    plt.plot(training_history['val_f1_scores'], label='F1 Score', color='green')
    plt.title('Validation F1 Score (Macro)')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 4, 4)
    plt.plot([x * 100 for x in training_history['val_signal_ratios']], label='Signal Ratio', color='purple')
    plt.title('Validation Signal Ratio')
    plt.xlabel('Epoch')
    plt.ylabel('Signal Ratio (%)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('multitimeframe_training_history_v04.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    plt.figure(figsize=(10, 8))
    sns.heatmap(lstm_eval['confusion_matrix'], annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix - Multi-Timeframe Model V04')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.xticks([0.5, 1.5, 2.5], ['Down', 'Hold', 'Up'])
    plt.yticks([0.5, 1.5, 2.5], ['Down', 'Hold', 'Up'])
    plt.savefig('multitimeframe_confusion_matrix_v04.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # ëª¨ë¸ ì €ì¥
    logger.info("\n4. ëª¨ë¸ ì €ì¥...")
    lstm_model.save_model('multitimeframe_bilstm_crypto_model_v04.pth')
    with open('multitimeframe_preprocessor_v04.pkl', 'wb') as f:
        pickle.dump(preprocessor, f)
    
    # ì €ì¥ëœ ë°ì´í„° ì •ë³´ ê¸°ë¡
    model_info = {
        'model_version': 'V04',
        'features': data_splits['feature_names'],
        'num_features': input_size,
        'threshold': preprocessor.threshold,
        'training_coins': COINS,
        'validation_coin': VAL_COIN,
        'date_range': f"{START_DATE} to {END_DATE}",
        'best_f1_score': max(training_history['val_f1_scores']),
        'final_accuracy': lstm_eval['accuracy'],
        'final_signal_ratio': lstm_eval['signal_ratio']
    }
    
    with open('model_info_v04.pkl', 'wb') as f:
        pickle.dump(model_info, f)
    
    logger.info("\n=== ë‹¤ì¤‘ ì‹œê°„ëŒ€ ëª¨ë¸ V04 í›ˆë ¨ ì™„ë£Œ! ===")
    logger.info(f"ìµœì¢… ê±°ë˜ ì‹ í˜¸: {lstm_eval['signal_count']}ê°œ ({lstm_eval['signal_ratio']:.2%})")
    logger.info(f"íŠ¹ì§•: 1ë¶„ë´‰ MACD + 5ë¶„ë´‰ MACD + ë‹¤ì´ë²„ì „ìŠ¤")


if __name__ == "__main__":
    main()