"""
build_dataset.py - í”¼ì²˜ ë° ë¼ë²¨ ìƒì„± ë„êµ¬ (ë²¡í„°í™” ìµœì í™”)

CLI ì‚¬ìš© ì˜ˆì‹œ:

# Classic ë°©ì‹ (ë©€í‹° íƒ€ì„í”„ë ˆì„ + ê¸°ìˆ ì  ì§€í‘œ)
python tools/build_dataset.py `
  --mode classic `
  --in data/raw/krw_btc_1m_30d.parquet `
  --out data/classic/dataset_mtf_30d_h2_0.0005.parquet `
  --horizon 2 `
  --up 0.0005 `
  --dn -0.0005 `
  --tfs 3,5

# Sequence ë°©ì‹ (ê¶Œì¥ - ìµœê·¼ Në´‰ íŒ¨í„´)
python tools/build_dataset.py `
  --mode seq `
  --in data/raw/krw_btc_1m_180d.parquet `
  --out data/seq/dataset_seq_180d_n20_h1.parquet `
  --n_steps 20 `
  --horizon 1 `
  --up 0.0005 `
  --dn -0.0005 `
  --ta

# CSV íŒŒì¼ë„ í•¨ê»˜ ì €ì¥ (ë””ë²„ê¹…/ë¶„ì„ìš©)
python tools/build_dataset.py `
  --mode classic `
  --in data/raw/krw_btc_1m_180d.parquet `
  --out data/classic/dataset.parquet `
  --horizon 20 `
  --up 0.001 `
  --dn -0.001 `
  --save_csv

ì‚¬ìš©ë²•:
â€¢ collect.pyë¡œ ìˆ˜ì§‘í•œ 1ë¶„ë´‰ ë°ì´í„°ë¥¼ ML í›ˆë ¨ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤
â€¢ Classic: ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ì˜ˆì¸¡ (í˜„ì¬ ì§€í‘œ â†’ horizon ë¶„ í›„ ì˜ˆì¸¡)
â€¢ Sequence: íŒ¨í„´ ê¸°ë°˜ ì˜ˆì¸¡ (ìµœê·¼ Në´‰ íŒ¨í„´ â†’ horizon ë¶„ í›„ ì˜ˆì¸¡)
â€¢ --save_csv: CSV íŒŒì¼ë„ í•¨ê»˜ ì €ì¥ (ì—‘ì…€/ë””ë²„ê¹… ëª©ì )

ë§¤ê°œë³€ìˆ˜ ê°€ì´ë“œ:
â€¢ --horizon: ì˜ˆì¸¡ ì‹œì  (1=ë‹¤ìŒë´‰, 3=3ë¶„ í›„, 5=5ë¶„ í›„) â€» ê¶Œì¥: 1-5ë¶„
â€¢ --up/--dn: ë¼ë²¨ë§ ì„ê³„ê°’
  - Â±0.0005 (Â±0.05%): ê· í˜•ì¡íŒ ë¶„í¬ (ë§¤ìˆ˜36%, ë³´í•©28%, ë§¤ë„36%)
  - Â±0.001 (Â±0.1%): ì ë‹¹í•œ ì„ íƒì„± (ë§¤ìˆ˜26%, ë³´í•©49%, ë§¤ë„25%)
  - Â±0.003 (Â±0.3%): ë†’ì€ ì„ íƒì„± (ë§¤ìˆ˜7%, ë³´í•©87%, ë§¤ë„7%) â€» ë¶ˆê· í˜•
â€¢ --n_steps: ì…ë ¥ íŒ¨í„´ ê¸¸ì´ (10=10ë¶„ë´‰, 20=20ë¶„ë´‰, 30=30ë¶„ë´‰)
â€¢ --tfs: Classic ì „ìš© ë©€í‹° íƒ€ì„í”„ë ˆì„ (3,5 = 3ë¶„ë´‰+5ë¶„ë´‰ ì¶”ê°€)
â€¢ --ta: Sequence ì „ìš© ê¸°ìˆ ì  ì§€í‘œ í¬í•¨ (MACD, RSI, BB, ATR ë“±)

ì„±ëŠ¥ ìµœì í™”:
â€¢ ë²¡í„°í™”ëœ ìœˆë„ìš° ìƒì„±ìœ¼ë¡œ 5-10ë°° ì†ë„ í–¥ìƒ
â€¢ 420ë¶„ ì›Œë°ì—…ìœ¼ë¡œ ì•ˆì •í™”ëœ ì§€í‘œê°’ ë³´ì¥
â€¢ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ë¬´ê´€ ë™ì¼ ì§€í‘œê°’ ìƒì„±
â€¢ GPU ì¹œí™”ì  ë°ì´í„° êµ¬ì¡° ìƒì„±
"""

import argparse
import os, sys
import pandas as pd
import numpy as np
from typing import List

# ---- ensure project root on sys.path so "ml" is importable ----
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.append(ROOT)

# Try to import indicators if present; otherwise use local fallbacks
try:
    from ml.indicators import ema as _ema, rsi as _rsi, macd as _macd
    HAVE_ML = True
except Exception:
    HAVE_ML = False

def ema(series: pd.Series, period: int) -> pd.Series:
    if HAVE_ML:
        return _ema(series, period)
    return series.ewm(span=period, adjust=False, min_periods=1).mean()

def rsi(series: pd.Series, period: int=14) -> pd.Series:
    """Wilder's RSI - ì—…ë¹„íŠ¸/TradingView í‘œì¤€"""
    if HAVE_ML:
        return _rsi(series, period)
    
    delta = series.diff()
    gains = delta.where(delta > 0, 0)
    losses = -delta.where(delta < 0, 0)
    
    # Wilder's Smoothing (RMA)
    # alpha = 1/period for Wilder's method
    avg_gains = gains.ewm(alpha=1/period, adjust=False, min_periods=period).mean()
    avg_losses = losses.ewm(alpha=1/period, adjust=False, min_periods=period).mean()
    
    rs = avg_gains / avg_losses
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50.0)

def macd(series: pd.Series, fast:int=12, slow:int=26, signal:int=9):
    if HAVE_ML:
        return _macd(series, fast, slow, signal)
    ema_fast = ema(series, fast)
    ema_slow = ema(series, slow)
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False, min_periods=1).mean()
    hist = macd_line - signal_line
    return macd_line, signal_line, hist

def bollinger(series: pd.Series, period:int=20, stds:float=2.0):
    ma = series.rolling(window=period, min_periods=1).mean()
    sd = series.rolling(window=period, min_periods=1).std(ddof=1)
    upper = ma + stds*sd
    lower = ma - stds*sd
    width = (upper - lower) / ma.replace(0, np.nan)
    return ma, upper, lower, width

def atr(high: pd.Series, low: pd.Series, close: pd.Series, period:int=14):
    """Wilder's ATR"""
    prev_close = close.shift(1)
    tr1 = (high - low).abs()
    tr2 = (high - prev_close).abs()
    tr3 = (low - prev_close).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    
    # Wilder's smoothing
    return tr.ewm(alpha=1/period, adjust=False, min_periods=period).mean()

# -----------------------
# Feature builders (original, classic mode)
# -----------------------
def add_features_ohlcv(df: pd.DataFrame, prefix: str="") -> pd.DataFrame:
    out = df.copy()
    out[f"{prefix}ret_1"]  = out["close"].pct_change()
    out[f"{prefix}ret_5"]  = out["close"].pct_change(5)
    out[f"{prefix}ret_15"] = out["close"].pct_change(15)
    out[f"{prefix}vol_20"] = out[f"{prefix}ret_1"].rolling(20, min_periods=5).std(ddof=1)
    out[f"{prefix}vol_60"] = out[f"{prefix}ret_1"].rolling(60, min_periods=10).std(ddof=1)

    out[f"{prefix}ema_12"] = ema(out["close"], 12)
    out[f"{prefix}ema_26"] = ema(out["close"], 26)
    m, s, h = macd(out["close"], 12, 26, 9)
    out[f"{prefix}macd"] = m
    out[f"{prefix}macd_sig"] = s
    out[f"{prefix}macd_hist"] = h
    out[f"{prefix}rsi_14"] = rsi(out["close"], 14)

    bb_ma, bb_up, bb_dn, bb_width = bollinger(out["close"], 20, 2.0)
    out[f"{prefix}bb_ma"]    = bb_ma
    out[f"{prefix}bb_up"]    = bb_up
    out[f"{prefix}bb_dn"]    = bb_dn
    out[f"{prefix}bb_width"] = bb_width

    out[f"{prefix}atr_14"] = atr(out["high"], out["low"], out["close"], 14)

    out[f"{prefix}dist_bb_up"] = (out[f"{prefix}bb_up"] - out["close"]) / out[f"{prefix}bb_up"].replace(0, np.nan)
    out[f"{prefix}dist_bb_dn"] = (out["close"] - out[f"{prefix}bb_dn"]) / out[f"{prefix}bb_dn"].replace(0, np.nan)
    out[f"{prefix}ema_gap"]    = (out[f"{prefix}ema_12"] - out[f"{prefix}ema_26"]) / out[f"{prefix}ema_26"].replace(0, np.nan)
    return out

def resample_ohlcv(df_1m: pd.DataFrame, m: int) -> pd.DataFrame:
    rule = f"{m}min"
    agg = {"open":"first","high":"max","low":"min","close":"last","volume":"sum","value":"sum"}
    r = df_1m.resample(rule, label="right", closed="right").agg(agg).dropna(subset=["close"])
    return r

def add_features_1m(df: pd.DataFrame) -> pd.DataFrame:
    df = df.sort_values("timestamp").reset_index(drop=True)
    df["ret_1"]     = df["close"].pct_change()
    df["logret_1"]  = np.log(df["close"]).diff()
    df["ret_5"]     = df["close"].pct_change(5)
    df["ret_15"]    = df["close"].pct_change(15)
    df["vol_20"]    = df["ret_1"].rolling(20, min_periods=5).std(ddof=1)
    df["vol_60"]    = df["ret_1"].rolling(60, min_periods=10).std(ddof=1)

    df["ema_12"]    = ema(df["close"], 12)
    df["ema_26"]    = ema(df["close"], 26)
    macd_line, macd_sig, macd_hist = macd(df["close"], 12, 26, 9)
    df["macd"]      = macd_line
    df["macd_sig"]  = macd_sig
    df["macd_hist"] = macd_hist
    df["rsi_14"]    = rsi(df["close"], 14)

    bb_ma, bb_up, bb_dn, bb_width = bollinger(df["close"], 20, 2.0)
    df["bb_ma"]     = bb_ma
    df["bb_up"]     = bb_up
    df["bb_dn"]     = bb_dn
    df["bb_width"]  = bb_width

    df["atr_14"]    = atr(df["high"], df["low"], df["close"], 14)

    df["dist_bb_up"] = (df["bb_up"] - df["close"]) / df["bb_up"].replace(0, np.nan)
    df["dist_bb_dn"] = (df["close"] - df["bb_dn"]) / df["bb_dn"].replace(0, np.nan)
    df["ema_gap"]    = (df["ema_12"] - df["ema_26"]) / df["ema_26"].replace(0, np.nan)
    return df

# -----------------------
# Classic labeling (horizon k)
# -----------------------
def add_labels(df: pd.DataFrame, horizon:int=20, up:float=0.003, dn:float=-0.003):
    future = df["close"].shift(-horizon)
    df["fwd_ret"] = (future - df["close"]) / df["close"]
    cond_up = df["fwd_ret"] >= up
    cond_dn = df["fwd_ret"] <= dn
    df["label"] = 0
    df.loc[cond_up, "label"] = 1
    df.loc[cond_dn, "label"] = -1
    df = df.iloc[:-horizon].copy()
    return df

# -----------------------
# NEW: Sequence(flatten) builder
# -----------------------
def build_seq_flat(
    df_in: pd.DataFrame,
    n_steps:int=10,
    use_ta:bool=False,
    horizon:int=1,
    up:float=0.0,
    dn:float=0.0,
    warmup_minutes:int=420
) -> pd.DataFrame:
    """
    ìµœê·¼ n_stepsê°œì˜ 1ë¶„ë´‰(OHLCV + ì„ íƒì§€í‘œ)ì„ í•œ í–‰ì— 'í¼ì³ì„œ' ë‹´ê³ ,
    ë‹¤ìŒ horizon(ê¸°ë³¸ 1) ë¶„ ë’¤ ë°©í–¥ ë¼ë²¨ì„ ë¶™ì¸ë‹¤.
    ì„ê³„(up/dn)=0ì´ë©´ ë‹¨ìˆœ ë‹¤ìŒë´‰ ìƒìŠ¹/í•˜ë½(=0 ì„ê³„) ë¼ë²¨.
    
    ì„±ëŠ¥ ìµœì í™”: ê¸°ìˆ ì  ì§€í‘œëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í•œ ë²ˆë§Œ ê³„ì‚°í•˜ê³ ,
    ê° ìœˆë„ìš°ì—ì„œëŠ” ë¯¸ë¦¬ ê³„ì‚°ëœ ì§€í‘œ ê°’ë“¤ì„ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ì‚¬ìš©
    """
    required = ["open","high","low","close","volume","value","timestamp"]
    for c in required:
        if c not in df_in.columns:
            raise ValueError(f"Input parquet must contain '{c}'")

    df = df_in.copy().sort_values("timestamp").reset_index(drop=True)

    # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ê¸°ìˆ ì  ì§€í‘œë¥¼ í•œ ë²ˆë§Œ ê³„ì‚° (ì„±ëŠ¥ ìµœì í™”)
    if use_ta:
        print("[seq] ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...")
        df = add_features_1m(df)

    # ë‹¤ìŒ horizon ë¶„ ë’¤ ìˆ˜ìµë¥ /ë¼ë²¨ ê³„ì‚°
    future = df["close"].shift(-horizon)
    fwd_ret = (future - df["close"]) / df["close"]
    if up == 0.0 and dn == 0.0:
        labels = np.where(fwd_ret > 0, 1, np.where(fwd_ret < 0, -1, 0))
    else:
        labels = np.zeros(len(df), dtype=int)
        labels[fwd_ret >= up] = 1
        labels[fwd_ret <= dn] = -1

    # ìœˆë„ìš°ì— í¬í•¨í•  ì»¬ëŸ¼ ê²°ì •
    base_cols = ["open","high","low","close","volume","value"]
    feat_cols = base_cols.copy()
    if use_ta:
        # ê¸°ìˆ ì  ì§€í‘œ ì»¬ëŸ¼ ì¶”ê°€ (ì´ë¯¸ ê³„ì‚° ì™„ë£Œë¨)
        extra = [c for c in df.columns if c not in set(base_cols + ["timestamp","timestamp_kst","fwd_ret","label"])]
        extra = [c for c in extra if pd.api.types.is_numeric_dtype(df[c])]
        feat_cols += extra

    print(f"[seq] {len(feat_cols)}ê°œ í”¼ì²˜ë¡œ {n_steps}ë´‰ ìœˆë„ìš° ìƒì„± ì¤‘...")
    
    # ğŸ”¥ ì›Œë°ì—… ì ìš©: 420ë¶„ (7ì‹œê°„) ì œê±°ë¡œ ì•ˆì •í™”ëœ ì§€í‘œê°’ ë³´ì¥
    if len(df) <= warmup_minutes:
        raise ValueError(f"ë°ì´í„°ê°€ ì›Œë°ì—… ê¸°ê°„({warmup_minutes}ë¶„)ë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {warmup_minutes + n_steps + horizon}ë¶„ í•„ìš”")
    
    print(f"[seq] ì›Œë°ì—… {warmup_minutes}ë¶„ ì œê±° (ì§€í‘œ ì•ˆì •í™”)")
    
    # ì›Œë°ì—… ì ìš©ëœ ìœˆë„ìš° ì¸ë±ìŠ¤ ê³„ì‚°
    start_idx = warmup_minutes + n_steps - 1  # ì›Œë°ì—… + ìœˆë„ìš° ì‹œì‘ì 
    end_idx = len(df) - horizon                # ë¼ë²¨ë§ ê°€ëŠ¥í•œ ë§ˆì§€ë§‰ ì 
    
    if start_idx >= end_idx:
        raise ValueError(f"ì›Œë°ì—… í›„ ë°ì´í„° ë¶€ì¡±: start={start_idx}, end={end_idx}. ë” ê¸´ ë°ì´í„° í•„ìš”")
    
    total_windows = end_idx - start_idx
    print(f"[seq] ì›Œë°ì—… í›„ {total_windows}ê°œ ìœˆë„ìš° ë²¡í„°í™” ì²˜ë¦¬ ì¤‘...")
    
    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¯¸ë¦¬ í• ë‹¹
    result_data = {}
    
    # ì›Œë°ì—… ì ìš©ëœ ìœˆë„ìš° ì¸ë±ìŠ¤ ìƒì„± (ë²¡í„°í™”)
    window_indices = np.arange(start_idx, end_idx)  # ì›Œë°ì—… í›„ ì¤‘ì‹¬ ì¸ë±ìŠ¤
    
    # ë©”íƒ€ë°ì´í„° ë²¡í„°í™” ìƒì„±
    result_data["timestamp"] = df.iloc[window_indices]["timestamp"].values
    result_data["close"] = df.iloc[window_indices]["close"].astype(float).values
    result_data["fwd_ret"] = fwd_ret.iloc[window_indices].astype(float).values
    result_data["label"] = labels[window_indices].astype(int)
    
    # ğŸš€ í•µì‹¬ ìµœì í™”: ëª¨ë“  í”¼ì²˜ì˜ ëª¨ë“  time stepì„ í•œë²ˆì— ìƒì„±
    df_array = df[feat_cols].values  # ì „ì²´ ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ
    
    for step in range(n_steps):
        step_name = f"t{-(n_steps-1-step)}"
        step_indices = window_indices - (n_steps - 1 - step)
        
        # ê° í”¼ì²˜ë³„ ë²¡í„°í™” ìŠ¬ë¼ì´ì‹±
        for col_idx, col in enumerate(feat_cols):
            if step == 0 and col_idx % 5 == 0:
                progress = (col_idx / len(feat_cols)) * 100
                print(f"[seq] ì§„í–‰ë¥ : {progress:.1f}% ({col_idx}/{len(feat_cols)} í”¼ì²˜)")
            
            feature_name = f"{col}_{step_name}"
            # ë²¡í„°í™”ëœ ì¸ë±ì‹± (ì¤‘ì²© ë£¨í”„ ëŒ€ì‹ )
            result_data[feature_name] = df_array[step_indices, col_idx].astype(float)

    print(f"[seq] ì´ {total_windows}ê°œ ìœˆë„ìš° ìƒì„± ì™„ë£Œ")
    out = pd.DataFrame(result_data)
    
    # ë¼ë²¨ì€ ì •ìˆ˜ë¡œ ìœ ì§€ (train.py í˜¸í™˜ì„±)
    # ì •ìˆ˜ ë¼ë²¨: -1=short, 0=flat, 1=long
    out["label"] = out["label"].astype(int)
    
    # ì›Œë°ì—…/NA ì œê±°
    out = out.dropna(axis=1, how="all")
    # í´ë˜ìŠ¤ ê·¹ë‹¨ì  ë¶ˆê· í˜• ë°©ì§€ìš© ê°„ë‹¨ í•„í„°(ì„ íƒ): í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
    # out = out[(out["label"]!=0) | (out["fwd_ret"].abs()>1e-8)]

    return out

# -----------------------
# Multi-TF (classic) helpers
# -----------------------
def parse_tfs(s: str) -> List[int]:
    if not s:
        return []
    return [int(x.strip()) for x in s.split(",") if x.strip()]

# ì¤‘ë³µ í•¨ìˆ˜ ì œê±°ë¨ (ìœ„ìª½ì— ì´ë¯¸ ì •ì˜ë¨)

# -----------------------
# Classic builder (original)
# -----------------------
def build_classic(inp_path: str, out_path: str, horizon:int, up:float, dn:float, tfs: List[int], warmup_minutes:int=420, save_csv:bool=False):
    df = pd.read_parquet(inp_path)
    if "timestamp" not in df.columns:
        raise ValueError("Input parquet must contain 'timestamp' column.")

    df = df.drop_duplicates(subset=["timestamp"]).sort_values("timestamp").reset_index(drop=True)

    # timestampëŠ” ì´ë¯¸ KST (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
    ts = pd.to_datetime(df["timestamp"])
    if ts.dt.tz is None:
        # tz-naiveë©´ KSTë¡œ ê°„ì£¼
        ts = ts.dt.tz_localize("Asia/Seoul")
    base_1m = df.set_index(ts)[["open","high","low","close","volume","value"]].copy()

    feat_1m = add_features_1m(df.copy())

    if tfs:
        merged = feat_1m.copy()
        # timestampëŠ” ì´ë¯¸ KST
        merged["timestamp"] = pd.to_datetime(merged["timestamp"])
        if merged["timestamp"].dt.tz is None:
            merged["timestamp"] = merged["timestamp"].dt.tz_localize("Asia/Seoul")
        left_df = merged[["timestamp"]].copy().sort_values("timestamp").reset_index(drop=True)

        for m in tfs:
            tf = resample_ohlcv(base_1m, m)
            tf_feat = add_features_ohlcv(tf, prefix=f"m{m}_")
            right_df = tf_feat.copy()
            # KST ìœ ì§€
            right_df["ts"] = right_df.index
            if right_df["ts"].dt.tz != merged["timestamp"].dt.tz:
                right_df["ts"] = right_df["ts"].dt.tz_convert("Asia/Seoul")
            right_df = right_df.sort_index().reset_index(drop=True)

            aligned = pd.merge_asof(
                left_df.sort_values("timestamp"),
                right_df.sort_values("ts"),
                left_on="timestamp",
                right_on="ts",
                direction="backward",
                allow_exact_matches=True,
            )
            aligned = aligned.drop(columns=["open","high","low","close","volume","value","ts"], errors="ignore")
            merged = pd.concat([merged.reset_index(drop=True), aligned.drop(columns=["timestamp"], errors="ignore")], axis=1)
    else:
        merged = feat_1m

    # ğŸ”¥ ì›Œë°ì—… ì ìš©: 420ë¶„ (7ì‹œê°„) ì œê±°ë¡œ ì•ˆì •í™”ëœ ì§€í‘œê°’ ë³´ì¥
    if len(merged) <= warmup_minutes:
        raise ValueError(f"ë°ì´í„°ê°€ ì›Œë°ì—… ê¸°ê°„({warmup_minutes}ë¶„)ë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {warmup_minutes + horizon}ë¶„ í•„ìš”")
    
    print(f"[classic] ì›Œë°ì—… {warmup_minutes}ë¶„ ì œê±° (ì§€í‘œ ì•ˆì •í™”)")
    merged = merged.iloc[warmup_minutes:].copy()
    merged = add_labels(merged, horizon=horizon, up=up, dn=dn)

    keep_core = ["ret_1","ema_12","ema_26","macd","rsi_14","bb_ma","atr_14","fwd_ret","label"]
    core_exists = [c for c in keep_core if c in merged.columns]
    if core_exists:
        merged = merged.dropna(subset=core_exists)

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    merged.to_parquet(out_path, index=False)
    print("dataset rows:", len(merged))
    
    # CSV ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ CSV ì €ì¥
    if save_csv:
        csv_path = out_path.replace('.parquet', '.csv')
        merged.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"[CSV ì €ì¥] {csv_path}")
    
    if len(merged):
        prop = (merged["label"].value_counts(normalize=True).sort_index()
                .rename({-1:"short",0:"flat",1:"long"}))
        print(prop)
    else:
        print("Empty dataset after processing.")

# -----------------------
# Sequence builder (new)
# -----------------------
def build_seq(inp_path: str, out_path: str, n_steps:int, ta:bool, horizon:int, up:float, dn:float, warmup_minutes:int=420, save_csv:bool=False):
    df = pd.read_parquet(inp_path)
    if "timestamp" not in df.columns:
        raise ValueError("Input parquet must contain 'timestamp' column.")
    df = df.drop_duplicates(subset=["timestamp"]).sort_values("timestamp").reset_index(drop=True)
    out = build_seq_flat(df, n_steps=n_steps, use_ta=ta, horizon=horizon, up=up, dn=dn, warmup_minutes=warmup_minutes)

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    out.to_parquet(out_path, index=False)
    print("dataset rows:", len(out))
    
    # CSV ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ CSV ì €ì¥
    if save_csv:
        csv_path = out_path.replace('.parquet', '.csv')
        out.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"[CSV ì €ì¥] {csv_path}")
    
    if len(out):
        prop = (out["label"].value_counts(normalize=True).sort_index()
                .rename({-1:"short",0:"flat",1:"long"}))
        print(prop)
    else:
        print("Empty dataset after processing.")

# -----------------------
# CLI
# -----------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["classic","seq"], default="classic",
                    help="classic: ê¸°ì¡´ ì§€í‘œ+horizon, seq: ìµœê·¼ Në´‰ í¼ì¹¨(ë‹¤ìŒ 1ë´‰)")
    ap.add_argument("--in", dest="inp", required=True, help="input parquet from tools/collect.py (1m OHLCV)")
    ap.add_argument("--out", dest="out", required=True, help="output dataset parquet")

    # classic params
    ap.add_argument("--horizon", type=int, default=20, help="label horizon in minutes (classic/seq ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)")
    ap.add_argument("--up", type=float, default=0.003, help="upper threshold for label")
    ap.add_argument("--dn", type=float, default=-0.003, help="lower threshold for label")
    ap.add_argument("--tfs", type=str, default="", help="classic ì „ìš©: '3,5,15'")

    # seq params
    ap.add_argument("--n_steps", type=int, default=20, help="seq ì „ìš©: ìµœê·¼ ëª‡ ë´‰ì„ í¼ì¹ ì§€ (ê¶Œì¥: 10~60)")
    ap.add_argument("--ta", action="store_true", help="seq ì „ìš©: OHLCV ì™¸ì— ì„ íƒ ì§€í‘œ(EMA/RSI/MACD/BB/ATR)ë„ í¬í•¨")
    
    # ê³µí†µ íŒŒë¼ë¯¸í„°
    ap.add_argument("--warmup_minutes", type=int, default=420, help="ì§€í‘œ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì›Œë°ì—… ê¸°ê°„(ë¶„) [ê¸°ë³¸ê°’: 420]")
    ap.add_argument("--save_csv", action="store_true", help="ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œë„ ì €ì¥ (ë””ë²„ê¹…/ë¶„ì„ìš©)")

    args = ap.parse_args()

    if args.mode == "classic":
        tfs = parse_tfs(args.tfs)
        if tfs:
            print(f"[build/classic] multi-TF: {tfs} warmup={args.warmup_minutes} save_csv={args.save_csv}")
        build_classic(args.inp, args.out, args.horizon, args.up, args.dn, tfs, args.warmup_minutes, args.save_csv)
    else:
        print(f"[build/seq] n_steps={args.n_steps} ta={args.ta} horizon={args.horizon} up={args.up} dn={args.dn} warmup={args.warmup_minutes} save_csv={args.save_csv}")
        build_seq(args.inp, args.out, args.n_steps, args.ta, args.horizon, args.up, args.dn, args.warmup_minutes, args.save_csv)

if __name__ == "__main__":
    main()
